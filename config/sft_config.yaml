# SFT (Supervised Fine-Tuning) Configuration
# ==========================================

# Model
model_name: "google/medgemma-4b-it"
audio_encoder_path: "checkpoints/mae/audio_encoder.pt"  # From MAE pretraining

# Audio Encoder (should match MAE config)
audio_input_dim: 128
audio_encoder_dim: 512
audio_encoder_layers: 6
audio_encoder_heads: 8
audio_max_seq_len: 64

# Projector
projector_type: "mlp"  # Options: mlp, qformer, perceiver
projector_hidden_dim: null  # Auto-computed if null
projector_num_layers: 2
num_audio_tokens: 8  # For qformer/perceiver

# LoRA Configuration
use_lora: true
lora_r: 16
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Training
batch_size: 4
gradient_accumulation_steps: 4  # Effective batch size = 16
num_epochs: 3
learning_rate: 2.0e-4
warmup_steps: 100
max_seq_length: 512

# Data
data_path: "data/processed"

# Output
output_dir: "checkpoints/sft"

# Quantization
use_4bit: true

# Misc
seed: 42
use_wandb: true
